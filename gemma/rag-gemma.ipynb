{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import fitz\n",
    "from spacy.lang.en import English # Create sentence barrier\n",
    "from sentence_transformers import SentenceTransformer, util # The embedding model and the metric model to find meaning (similarity search between\n",
    "# query meaning and the vector data in database from the the sentence chunks we embedded)\n",
    "# Let's leave the LLM import later so we can know for certain which one we'll use\n",
    "# Same with BytesAndBytesConfig to know if we need that for LLM, leave it for later, as our hardware isn't able to use flash attention 2\n",
    "from newspaper import Article\n",
    "import textwrap\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run a local RAG pipeline from scratch\n",
    "\n",
    "### We'll embed and use any LLM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RAG?\n",
    "\n",
    "Rag is Retrieval Augmented Generation\n",
    "\n",
    "The Goal Of Rag is take information and pass it into a LLM so it can generate outputs based on that information (especially to generate text transformer on the generate structure)\n",
    "\n",
    "* **Retrieval detailed** - Find relevant information given a query e.b (we'll build Nutri-chat) a model that reads and trains it's neural network with those 1200 pages in the textbook about nutrution, so those model will serve as more a guide on what it learned from the data recieved on textbook, to provide relevant information on that subject. By giving a set of question that the model can respond if we train it to on general dialogue. (what are the macronutrients and what do they) \n",
    "* **Retrieval** - Find relevant info given a query (input data), e.g. \"what are the macronutrients and what do they do>\" -> retrieves passages of text related to the macronutrients from a nutrition textbook. (goes to layer it retrieved from all and matches strings with the number it encoded and relations on those text)\n",
    "* **Augmented** - We want to take the relevant information and augment our input (prompt) \"make of greater importantance and match with the best relating text pattern output and bring it to an LLM with that relevant information.\n",
    "* **Generation detailed** - Take the first two steps R (retrieve input and text data relating towards input) and A (increase the importance of input with the relevant and most matched data retrieved making it augment the whole data) and pass them to an LLM for generative outputs. (to match the whole data importance to say in feedback this certain mode create \"asko\" the quotes are the augmentation if did from retrieving and pattern matching to input and the model from trained on dialogue to say proper general feedback to give this is this or that and know how to respond to question and understand mostly what's being typed completely)\n",
    "Generation - Take the first two steps and pass tho an LLm for generative output\n",
    "\n",
    "If you want to read where RAG came from, see the paper from Facebook AI - https://arxiv.org/abs/2005.11401\n",
    "\n",
    "What was said: *\"This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs. With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better for specific questions on topics relating to a subject, in comparison with gpt is good for general as it captures a vast amount of text than rather several to retrieve and remember them or let's for better \"use them\", that's why GPT gives very general answer due to it's model capability, however RAG uses a long range dependency like GPT but it's able to retrive more precise and detailed information that's right and not general to what's being asked because it take a large corpus of text, GPT doesn't have a built-in mechanism for retrieving specific information for a corpus.\n",
    "\n",
    "Diablo GPT has more dialogue generation or observes dialogue text that is able to respond more relevant information what's being asked on the context but not exact questions that are more detailed and exact to explain which to me is the whole point of AI for text to understand really deep and big topics like in doctors or engineers etc to understand information or even a user as a client, this removes even people out of this point but rather have model solve all of this which is a great approach incoming and start it up.\n",
    "\n",
    "GPT is trained on tons of terabytes of internet text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Rag\n",
    "\n",
    "The main goal of Rag is to improve the generation of outputs of LLMs.\n",
    "\n",
    "1. Prevent hallucinations - LLMs are incredibly good at generating good *looking* text, however, this text doesn't mean it's factual. Rage can help LLMs generate information based on relevant passages that are factual. (information stored somewhere to view what it sees by complete to know what your saying, GPT grabs bits and pieces what it sees and can't get you a straight answer to what you see)\n",
    "\n",
    "Give specific topics to answer and if needing to bounce around on different topics match you can have GPT mix what your trying to understand in a RAG model\n",
    "\n",
    "(GPT) can be very generic when asking questions like do this than this or this and those can help, but it doesn't tell you on things that can be a better solution to a deeper topic on understanding what it knows which is RAG can do to ask those questions.\n",
    "\n",
    "2. Word with custom data - Many base LLMs are trained with internet-scale data. This means they have a fairly good understanding of language in general. ( they read internet data) which is filled with human text of conversations, behaviours, types of way of asking questions and answering\n",
    "\n",
    "Augmentation would be to take passages from documents that you stored, put them into the query that's being asked. Than have a generative LLM to take those documents it has stored and retrieved from training data and stored data and format back to what it saw in a readable informative way for the user on query. This is the main point with **RAG**, it helps create specific responses based on specific documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can RAG be used for?\n",
    "\n",
    "* Customer support Q&A Chat - Treat your existing customer support documents as a resource and when a customer asks a question, you could have a retrieval system, retreive relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\".\n",
    "* Email chain analysis - Let's say you're a large insurance company and you have chains and chains of emails of customer claims. You could use a Rag pipeline to find relevant information from those emails (retreive to store on database, model to view patterns) and then use an LLM to process that information into structured data. (Augmentation)\n",
    "* Company internal documentation chat (to what you have in the inside)\n",
    "* Textbook Q&A - Let's say you're a nutrition student and you've got a 1200 page textbook read, you could build RAG pipeline to go through the textbook and find relevant passages to teh questions you have.\n",
    "\n",
    "Common theme: take your relevant documents to a query and process them with an LLM\n",
    "\n",
    "The LLM is really a calculator for words. To match and understand them for all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why local?\n",
    "\n",
    "Fun,\n",
    "\n",
    "privacy, speed and cost. Also way better expeirence and better user value. Pretrain the answer to whole text data, rather the model given to other companies and you fine tune it, by your own docs and needs but can't fully optimize on chat for generale small company to have on website to view what's around to best source to move to communicate and ask without browsing.As much but actually ask and it does everything it ask and actually communicate to view on site toc communicat on source the sources are placing the information, on website to rather have a bot that just can easily communicate practically to what the AI is doing to communicate with apps around through browser and also information on questions to answer on field to ask what i needs than rather in policy to need in help to solve than rather handle fast to more\n",
    "\n",
    "* Privacy - If you have private ducomentation, maybe you don't want to send that to an API. You want to setup an LLM and run it on your own hardware.\n",
    "* Speed - Whenever you use an API, you have to send some kind of data across the internet. Takes time. Running locally means we have instant access and not wait for transfers of data from the internet it needs retrieve.\n",
    "* Cost - If you own your hardware, the cost is paid for your self and have more flexibility on the things your using in (the hardware to better software) But overtime, you don't have to keep paying API fees.\n",
    "* No vendor lockin - If you run own software/hardware. If OpenAI/another large internet company shut down tomorrow, you can still run your business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 03:52:55 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti   WDDM  |   00000000:17:00.0  On |                  N/A |\n",
      "| 24%   30C    P8             30W /  250W |    1025MiB /  11264MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2188    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      2748    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A      5052    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     10888    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11004    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     11088    C+G   ...US\\ArmouryDevice\\asus_framework.exe      N/A      |\n",
      "|    0   N/A  N/A     11880    C+G   ...ecurityApp\\MicrosoftSecurityApp.exe      N/A      |\n",
      "|    0   N/A  N/A     14584    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17436    C+G   ...on\\123.0.2420.97\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     18556    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A     19660    C+G   ...8.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     21008    C+G   ...8.0_x64__t4vj0pshhgkwm\\Telegram.exe      N/A      |\n",
      "|    0   N/A  N/A     21112    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22336    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     29912    C+G   ...les\\Elgato\\CameraHub\\Camera Hub.exe      N/A      |\n",
      "|    0   N/A  N/A     29996    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     30432    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we're going to build\n",
    "\n",
    "* https://github.com/mrdbourke/simple-local-rag\n",
    "* https://whimsical.com/simple-local-rag-workflow-39kToR3yNf7E8kY4sS2tjV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing and embedding creation\n",
    "\n",
    "Collection of documents (data): This can be any sort docement, pdf, txt, html, anything the model will read and retrieve\n",
    "\n",
    "We can have this model read this pdf and learn pattern and augment relevant relations with data contained in it's models and easily view through locally on text. We can even do the same thing for the model to retrieve and augment data to the query from data on wikipedia.\n",
    "\n",
    "We will grab 10 sentences on the document and preprocess the text into smaller documents containing these few sentences that are used in LLM as **context** (our retrieval passages, we'll pass to the LLM later)\n",
    "\n",
    "Than embed those smaller chucnks of text (which means to convert that plane text and query into a numerical format for the model to calculate from sentence-transformers, these are options we can do with Hugging Face)\n",
    "\n",
    "After embedding we can store it in Pytorch (torch.tensor) and it's fast on 100k+ embeddings. We can also store it on a database to use it later\n",
    "\n",
    "Now in the visual graph above. You the user well ask a query \"What are the macronutrients of a mango, How can i use them?\" this query is than converted or embedded in the same format as the retrieved documents were to place it in the neural network to convert them into numeric representation.\n",
    "\n",
    "When you ask GPT: \"Hey, how much does an elephant weigh?\" it will turn that query into numeric format, than it will quiz it's weights which are also numeric, and find out if it's seen the numbers of that query before in it's own database and what it was trained on, it will find similiar numbers to the query from the data it stored and give an output back to what it found in similiar by asking to in a dialogue way not just a clear answer but say as human this is what it does why and such etc. \n",
    "\n",
    "In the neural network it takes the embedding by parts and cache the result to find the relation within the database and repeat until it found through all data relating towards query (it stores the query inside the database and the documents it found with the same type of weights relating to the in it and can find relations which will make to augmenet other wise it will respond with dialogue that it coudn't find it.)\n",
    "\n",
    "#### The retrieval part is when in finds the connect numeric data from it's own documents and data to the query through relations and weight size similiarly\n",
    "\n",
    "It finds numeric representatoon in it's database to match what the query is saying. IN the same phase to it does the augmentation to combine the query and what it found relating towards each other\n",
    "\n",
    "We take our question from the query and append the relevant passages to our text and we pass this text to a LLM\n",
    "\n",
    "\n",
    "It grabs the numeric relation to the sentences related towards question in query and the model chosses which of the options in relation to query has more relations to it. Which has a numeric relation, same weight to the question, and this process is done by the Large Language Model (LLM), which of the options related to query is more closely related numerically to it by each word and seen output relation. Which of these sentences and processes match are related most to what's being asked, such as what where and why, the one that is get's processed and formatted into back dialogue text what they asked and place it out as ouput\n",
    "\n",
    "it finds a page or paragraph relating towards problem:\n",
    "\n",
    "context1: one of the articles or paragraphs it found relating towards numercially seen in the input or query\n",
    "context2: another pargraph closely related to queries value and weight amount\n",
    "context3: another paragrahph related towards query value and weight amount\n",
    "\n",
    "based on which every has the closest numeric value to the query text numeric form is choosen for augmentation and ouput by the LLM\n",
    "\n",
    "After augmenting the LLM generates output based on context related to query and it shows it in the UI program where the query is being sent, and we can get an answer to where that answer it made came from and actually see oruselves whether the model has been incorrect or the sources valid, which is great as it makes it more solid\n",
    "\n",
    "The model will say the macronutrients are tied to this and here, this is where i got that information: on page 5 on the MacroNutrients sections, and also on page 22 on section bones, and page 53 on section eating etc it will give you the combination off results it found to give an answer from all areas on documents it did to augment process and place otput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to build NutriChat to \"chat\" with a textbook\n",
    "\n",
    "textbook here can be anything such as customer relation docs, data analytics documents, culture docs, research paper docs, customer service docs etc\n",
    "\n",
    "Specifically\n",
    "\n",
    "1. Open a PDF ducment (you could use almost any PDF here or even a colleciton PDF's).\n",
    "2. Format the text of the PDF textbook ready for an embedding model. (create chunks and get the sentences)\n",
    "3. Embed all of the chunks of text in the textbook and turn into numerical representations (embedding) which we can store for later.\n",
    "4. Build a retrieval system that uses vector search to find relevant chunk of text based on a query.\n",
    "5. Create a prompt that incorporates the retrieved pieces of text. (reformats or decodes into back a string or the type of data the query was passed)\n",
    "6. Generate an answer to a query based on the passages of the textbook with LLM.\n",
    "\n",
    "All locally!\n",
    "\n",
    "1. Steps 1-3: Document preprocessing and emedding creation.\n",
    "2. Steps 4-6: Search and answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document/text processing and embedding creation\n",
    "\n",
    "Ingredients: \n",
    "* PDF document of choice (noteL this could be almost any kind of docment, I've just chosen to focus on PDFs for now).\n",
    "* Embedding model of choice.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Import PDF document.\n",
    "2. Process text for embedding (e.g. split into chunks of sentences).\n",
    "3. Embed text chunks with embedding model.\n",
    "4. Save embeddings to file for later (embeddings will store on file for many years or until you lose your hard drive)\n",
    "\n",
    "### Import PDF Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We've got a PDF, let's open it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use the PyMuPDF library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a pdf ready and in a function we'll try to retreive the data inside including text, to be in right format, to get it all since it's a bit different in dealing with PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a token is a sub word piece of text, ex. \"hello, world!\" could be split into [\"hello\",\",\"world\",\"!\"] a token can be a whole word.\n",
    "\n",
    "part of a word or punctuation of characters. 1 token ~= 4 characters in english making 100 tokens if there is 75 words\n",
    "\n",
    "Text gets broken into tokens before being passed to a LLM.\n",
    "\n",
    "#### So a token is NOT a character but rather a word. \n",
    "\n",
    "It's a subpiece of the original word, so in tokenizing the word \"tokenization\" would look like \"to\" \"ken\" \"iza\" \"tion\" since where using a tokenizer that splits the original in 4 characters that it would place as a single token \n",
    "\n",
    "specifically 4 characters in a word is 1 token. Remember it's 'CHARACTERS' so it can be alphabetical or symbols such as [a, B, #, %, *. ^. /, ., z, q, `, -] etc\n",
    "\n",
    "Conversly if you were using a tokenizer that treats each word as a single token, then the number of tokens is equivalent to the number of words\n",
    "\n",
    "In our case if 100 tokens as 75 words than clearly we're sub splitting words so elephant can be \"ele\" \"phant\" it's splitting words into subword units, resulting in more tokens than the number of words, this a scenario used for subword tokenization, where words are broken down into smaller units to improve the models ability to handle rare words or out of vocabulary words such as slangs or sentences that don't connect grammicaly but can understand in general what there asking for importance.\n",
    "\n",
    "\n",
    "Here is a great doc explaining Tokenizer in depth by openai - https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the raw text that's placed in the query \n",
    "\n",
    "but when the words need to tokenized int he part of seperating them into sentencs to make chunks for models retrieval and comparison with data after the embeddment. The tokenization when it's converted into numbers look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This tokenization process is meant for the model to examine the text at a more granular level\n",
    "\n",
    "These words and tokens are using a token subword scheme such as Byte Pair Encoding or WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why would we care about the token count?\n",
    "\n",
    "Token count is important because:\n",
    "1. Embedding models don't deal with infinite tokens.\n",
    "2. LLMs don't deal with infinite tokens. (can't place all the text in our LLM and it would be computationally wastefull, since there is an amount of text that isn't use in all, that you need fo filter and sort)\n",
    "\n",
    "For example an embedding model may have been trained to embed sequences of 384 tokens into numerical space. (sentence-transformers `all-mpnet-base-v2`)\n",
    "\n",
    "we'll use hugging face sentence transformer - https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "and also from sbert.net sentence transformer - https://sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models in sentence transformer actually have a limit to the amount of tokens they can get, for example the `all-mpnet-base-v2` can only take about 384 max seqeunce token, anything more will get shrinked down making response unreliable if you need more spread out wide results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding model will turn a string of text into a numeric representation to be useful for machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for LLM's, they can't accept an infinite tokens in their context window\n",
    "\n",
    "You don't want to have the most tokens, but rather have the best token to gather from whole section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's research and find all relevant data online for our database in RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with `site pages` and `PDF's` from our **data-processing notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file from data processing that has the raw data formatted\n",
    "df = pd.read_csv(\"./data-processing/indata-processing/whole_sitepages_pdf_text_df.csv\")\n",
    "data_dict = df.to_dict(orient=\"records\")\n",
    "df = pd.DataFrame(df)\n",
    "data_dict = df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_or_page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>73.32</td>\n",
       "      <td>4040.39</td>\n",
       "      <td>631.01</td>\n",
       "      <td>40.29</td>\n",
       "      <td>1010.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>137.05</td>\n",
       "      <td>2386.37</td>\n",
       "      <td>366.26</td>\n",
       "      <td>62.59</td>\n",
       "      <td>596.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.00</td>\n",
       "      <td>2219.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>554.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.00</td>\n",
       "      <td>3878.00</td>\n",
       "      <td>584.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>969.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.00</td>\n",
       "      <td>5443.00</td>\n",
       "      <td>889.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>1360.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>566.00</td>\n",
       "      <td>13344.00</td>\n",
       "      <td>2273.00</td>\n",
       "      <td>1156.00</td>\n",
       "      <td>3336.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       link_or_page_number  page_char_count  page_word_count  \\\n",
       "count              2545.00          2545.00          2545.00   \n",
       "mean                 73.32          4040.39           631.01   \n",
       "std                 137.05          2386.37           366.26   \n",
       "min                   0.00             0.00             1.00   \n",
       "25%                   4.00          2219.00           350.00   \n",
       "50%                  12.00          3878.00           584.00   \n",
       "75%                  47.00          5443.00           889.00   \n",
       "max                 566.00         13344.00          2273.00   \n",
       "\n",
       "       page_sentence_count  page_token_count  \n",
       "count              2545.00           2545.00  \n",
       "mean                 40.29           1010.10  \n",
       "std                  62.59            596.59  \n",
       "min                   1.00              0.00  \n",
       "25%                  11.00            554.75  \n",
       "50%                  18.00            969.50  \n",
       "75%                  34.00           1360.75  \n",
       "max                1156.00           3336.00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's keep our formatted data into a dictionary that we can manipulate it easier\n",
    "\n",
    "We got our data on each key, let's add a sentenicer to create sentence barriers\n",
    "\n",
    "**Let's add SpaCy sentencizer** In our main jupyter notebook now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This another., The last sentence.]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add spacy sentencizer\n",
    "# Let's do a sample to get how this works\n",
    "nlp = English()\n",
    "\n",
    "nlp.add_pipe('sentencizer') # sentencizer is meant to segment the text when nlp(\"text.\") into a object making as sentences\n",
    "doc = nlp(\"This is a sentence. This another. The last sentence.\")\n",
    "assert len(list(doc.sents)) == 3 # assert is like an if condition, if this isn't exactly 3 sentences than it would raise an AssertionError\n",
    "# We want this condition because clearly we have 3 sentences but this is to make sure that sentencizer NLP is working to find sentences\n",
    "list(doc.sents) # in this list contains each sentences, this efficient for our formatting and dictionary. In a page contains all the sentences to it\n",
    "# but each sentences is seperated and indentified in our code by a NLP sentencizer\n",
    "# the doc.sents will simple make the doc object that was done when we sent that sentence string to nlp and it turned it into an object\n",
    "# really to process it than doc.sents will create barrier on only that doc object format that sentencizer understands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sentencizer is working, let's make a function\n",
    "\n",
    "Let's function both this sentencizer with the accuracte amount of total sentences and\n",
    "make the chunks of 10 sentences in 1 function.\n",
    "\n",
    "> Also let's allow only sentences containing more than 30 tokens, anything less will excluded\n",
    "\n",
    "We are removing the less 30 tokens on the chunks because this is were we will process it into the embedding. If we minimized on sentences it wouldn't be clear to the total sentences as text creating is not fully fixed  (some sentences bigger than other) but in total 10 sentences should contain an expected amount of characters in total. In our case really doing it on the chunking process just makes it more practical as that's the area that will finally be embedded. So we make sure after embedding that each chunk contains more than 30 tokens.\n",
    "\n",
    "Plus the NLP on sentencizer well sentencize even when it sees `1.` or `3.` since it ends in a period and has a space. That's why it's better after nlp and chunking to only allow > 30 size tokens on each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see if our nlp sentencizing works as it should for all of them, than afterwords we can proceed to place them in a dict --> chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further text processing (splitting pages into sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### splitting pages into group of 10 sentences\n",
    "\n",
    "Two ways to do this:\n",
    "1. We've done this by splitting on `\". \"`.\n",
    "2. We can do this with a NLP library such as spaCy https://spacy.io/usage and NLTK https://www.nltk.org/ (You need to preprocess text and prepare it for LLMs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This another sentence., I like elephants.]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Let's add a sentencizer pipeline, see \n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# add a sentencizer model to combine what it sees on the vocab in improvement from similiar models and combines\n",
    "\n",
    "# Create document instance as an example\n",
    "doc = nlp(\"This is a sentence. This another sentence. I like elephants.\")\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "# print out our sentences split\n",
    "list(doc.sents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see if our nlp sentencizing works as it should for all of them, than afterwords we can proceed to place them in a dict --> chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "\n",
    "def sentencizer_and_chunking(data_dict: list,\n",
    "                             num_sents_per_chunk: int=10):\n",
    "    \"\"\"\n",
    "    Sentencize your sentences, add them to dictionary with accurate total sentences,\n",
    "    chunk each sentences into 10 per group, add to dict again with right labels,\n",
    "    than exclude any chunks with less than 30 tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Let's make a list to contain all the seperated sentences for a sample\n",
    "    # Make sure all text here is string when it identifies it, for some reason the nlp when seeing numbers can detect floats or ints even if they were\n",
    "    # in dataframe into dict so convert tem here to str to be sure\n",
    "    nlp = English()\n",
    "    nlp.add_pipe('sentencizer')\n",
    "    for i in data_dict:\n",
    "        text = str(i['text'])\n",
    "        doc = nlp(text)\n",
    "        sentence = [str(sent) for sent in doc.sents]\n",
    "        i['sentence'] = sentence\n",
    "        i[\"page_sentence_count_spacy\"] = len(i['sentence'])\n",
    "\n",
    "        # Chunk sentences\n",
    "        # Split the list\n",
    "        # for each item in rane of 0 to the max length of items in sentence_list, iterate by a step of num_sents_per_chunk, than have it return\n",
    "        # the sentence position that has 10\n",
    "        sentence_list = i['sentence']\n",
    "        split_sentence_list = [sentence_list[i:i + num_sents_per_chunk] for i in range(0, len(sentence_list), num_sents_per_chunk)]\n",
    "        # Loop through the pages and split the sentences into chunks\n",
    "        i[\"sentence_chunk\"] = [chunk for chunk in split_sentence_list]\n",
    "        i['num_chunks'] = len(i[\"sentence_chunk\"])\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def chunks_and_tokenize(data_dict: list,\n",
    "                        token_amount: int):\n",
    "    \"\"\"\n",
    "    This will make a chunk_list filled of dictionaries, to make analyzes more practical.\n",
    "    It will exclude a token amount to allow only a set minimum # tokens in our chunks\n",
    "    \"\"\"\n",
    "\n",
    "    # Make chunk list(dict)\n",
    "    pages_and_chunks = []\n",
    "    for i in data_dict:\n",
    "        for sentence_chunk in i['sentence_chunk']:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"link_or_page_number\"] = i[\"link_or_page_number\"]\n",
    "            # Join sentences together as a paragpraph structure (as a chunk so it's just a single string than rather seperated in the sentence chunk all\n",
    "            # together)\n",
    "            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "            joined_sentence_chunk = re.sub(r'\\.([A-Z0-9])', r'. \\1', joined_sentence_chunk) # For any full stopped characters\n",
    "            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "            # Get stats for chunk dict\n",
    "            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")]) # Now it's seperated in the entire dictionary, that \n",
    "            # way calling each item is really each character before a space making it a word\n",
    "\n",
    "            # Let's only allow 30 tokens or above\n",
    "            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4\n",
    "            pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "    # Let's only allow 30 tokens above for each chunk\n",
    "    df = pd.DataFrame(pages_and_chunks)\n",
    "    pages_and_chunks_overmin_token_dict = df[df[\"chunk_token_count\"] > token_amount].to_dict(orient=\"records\")\n",
    "    \n",
    "    return pages_and_chunks, pages_and_chunks_overmin_token_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will count 20 even if there was 15 sentences since it rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this line of code: `sentence = [str(sent) for sent in doc.sents]` the doc.sents is to make the object nlp in sents barrier, once it does than each sentence inside that object will be make into string type and sentence is now the variable containing just the sentence for the page or link we're looking at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can analyze after sentencizing and chunking, too see if everything was good\n",
    "\n",
    "By seeing in dataframes but all of this after making sentences than chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_and_chunks = sentencizer_and_chunking(data_dict=data_dict,\n",
    "                                                num_sents_per_chunk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_token_dict, chunk_min_token = chunks_and_tokenize(data_dict=data_dict,\n",
    "                                                        token_amount=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze these chunks now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10321, 10077)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_token_dict), len(chunk_min_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the funtion grab the seperated sentences and add them to `data_dict` we're all those other keys are and get an accurate sentence count\n",
    "\n",
    "or you can add them to the dataframe `df` if that is more practical, as another column filled with that sentence content, make it drop the last sentences with new ones. This is so that we can actually chunk our sentences by 10 since it's unpredictable to do this without seperating them with NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_or_page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "      <td>2545.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>73.32</td>\n",
       "      <td>4040.39</td>\n",
       "      <td>631.01</td>\n",
       "      <td>40.29</td>\n",
       "      <td>1010.10</td>\n",
       "      <td>35.80</td>\n",
       "      <td>4.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>137.05</td>\n",
       "      <td>2386.37</td>\n",
       "      <td>366.26</td>\n",
       "      <td>62.59</td>\n",
       "      <td>596.59</td>\n",
       "      <td>46.43</td>\n",
       "      <td>4.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.00</td>\n",
       "      <td>2219.00</td>\n",
       "      <td>350.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>554.75</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.00</td>\n",
       "      <td>3878.00</td>\n",
       "      <td>584.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>969.50</td>\n",
       "      <td>18.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.00</td>\n",
       "      <td>5443.00</td>\n",
       "      <td>889.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>1360.75</td>\n",
       "      <td>33.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>566.00</td>\n",
       "      <td>13344.00</td>\n",
       "      <td>2273.00</td>\n",
       "      <td>1156.00</td>\n",
       "      <td>3336.00</td>\n",
       "      <td>392.00</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       link_or_page_number  page_char_count  page_word_count  \\\n",
       "count              2545.00          2545.00          2545.00   \n",
       "mean                 73.32          4040.39           631.01   \n",
       "std                 137.05          2386.37           366.26   \n",
       "min                   0.00             0.00             1.00   \n",
       "25%                   4.00          2219.00           350.00   \n",
       "50%                  12.00          3878.00           584.00   \n",
       "75%                  47.00          5443.00           889.00   \n",
       "max                 566.00         13344.00          2273.00   \n",
       "\n",
       "       page_sentence_count  page_token_count  page_sentence_count_spacy  \\\n",
       "count              2545.00           2545.00                    2545.00   \n",
       "mean                 40.29           1010.10                      35.80   \n",
       "std                  62.59            596.59                      46.43   \n",
       "min                   1.00              0.00                       1.00   \n",
       "25%                  11.00            554.75                      11.00   \n",
       "50%                  18.00            969.50                      18.00   \n",
       "75%                  34.00           1360.75                      33.00   \n",
       "max                1156.00           3336.00                     392.00   \n",
       "\n",
       "       num_chunks  \n",
       "count     2545.00  \n",
       "mean         4.06  \n",
       "std          4.64  \n",
       "min          1.00  \n",
       "25%          2.00  \n",
       "50%          2.00  \n",
       "75%          4.00  \n",
       "max         40.00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_sentences_and_chunks_df = pd.DataFrame(sentences_and_chunks)\n",
    "# pages_and_sentences_df.head()\n",
    "pages_sentences_and_chunks_df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pages_sentences_and_chunks_df[:2]\n",
    "data_dict[2]['num_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[2]['page_sentence_count_spacy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's make a seperate dictionary in the function called `chunk_dict`\n",
    "\n",
    "showing relevant information for our data analysis (we can in future add each step process in other variables in a OOP) after that than we can any chunk that has less than 30 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This chunking is correctly because it needs to find each chunk per page\n",
    "\n",
    "The total chunks can be really big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_token_dict[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_or_page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10321.00</td>\n",
       "      <td>10321.00</td>\n",
       "      <td>10321.00</td>\n",
       "      <td>10321.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94.93</td>\n",
       "      <td>992.67</td>\n",
       "      <td>152.71</td>\n",
       "      <td>248.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>152.74</td>\n",
       "      <td>879.38</td>\n",
       "      <td>146.31</td>\n",
       "      <td>219.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.00</td>\n",
       "      <td>467.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>116.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.00</td>\n",
       "      <td>642.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>160.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>96.00</td>\n",
       "      <td>1315.00</td>\n",
       "      <td>199.00</td>\n",
       "      <td>328.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>566.00</td>\n",
       "      <td>7536.00</td>\n",
       "      <td>1441.00</td>\n",
       "      <td>1884.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       link_or_page_number  chunk_char_count  chunk_word_count  \\\n",
       "count             10321.00          10321.00          10321.00   \n",
       "mean                 94.93            992.67            152.71   \n",
       "std                 152.74            879.38            146.31   \n",
       "min                   0.00              1.00              1.00   \n",
       "25%                   7.00            467.00             68.00   \n",
       "50%                  16.00            642.00             95.00   \n",
       "75%                  96.00           1315.00            199.00   \n",
       "max                 566.00           7536.00           1441.00   \n",
       "\n",
       "       chunk_token_count  \n",
       "count           10321.00  \n",
       "mean              248.17  \n",
       "std               219.85  \n",
       "min                 0.25  \n",
       "25%               116.75  \n",
       "50%               160.50  \n",
       "75%               328.75  \n",
       "max              1884.00  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(chunk_token_dict)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our chunks have a low token count\n",
    "\n",
    "Let's add a minimum amount to keep a worthy quantity of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in df[df[\"chunk_token_count\"] <= 30].sample(10).iterrows():\n",
    "#     print(f'Page Number or link: {row[1][\"link_or_page_number\"]} | Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:63fc1183-b1ab-4aae-95d7-679d57e556b6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These tokens are not reliably, they give either titles, captions or headers\n",
    "\n",
    "Let's change to only allow above 30 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's place this inside our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_min_token[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! Let's now finally embed our sentences into numeric form\n",
    "\n",
    "using `all-mpnet-base-v2` is seen as the best in sbert.net\n",
    "\n",
    "It has low memory size, A high embedding speed and semantic search. It's now the complete fastest in embed but all around it's better than the rest because there is another that has 1 digit above in speed for embedding but it takes long to cross semantic search and has a bigger size storage requirement which isn't worth it\n",
    "\n",
    "So we'll stick to `all-mpnet-base-v2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a function to just embed our data\n",
    "\n",
    "We keep this step seperate if we needed to manipulate anything additionally in the future with just our function for the NLP and chunking process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santi\\Deep-Learning\\rag\\medical-rag\\run-an-llm-from-scratch\\gemma\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\santi\\Deep-Learning\\rag\\medical-rag\\run-an-llm-from-scratch\\gemma\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Let's use the sentence_transformers for the embedding\n",
    "embedding_model = SentenceTransformer(model_name_or_path='all-mpnet-base-v2',\n",
    "                                      device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_data(chunk_data: list,\n",
    "                   batch_size: int=10,\n",
    "                   convert_to_tensors: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds the whole sentence_chunks inside the dictionary into\n",
    "    vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in chunk_data:\n",
    "        i[\"embeddings\"] = embedding_model.encode(i[\"sentence_chunk\"])\n",
    "\n",
    "    df = pd.DataFrame(chunk_data)\n",
    "    df.to_csv(\"./chunks_tokenized.csv\", index=False)\n",
    "    # Let's batch them into 32 to make the processing faster\n",
    "    # text_chunks = [i['sentence_chunk'] for i in chunk_min_token]\n",
    "\n",
    "    # text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "    #                                                batch_size=batch_size,\n",
    "    #                                                convert_to_tensor=True)\n",
    "    return chunk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I tried the batching but it made no difference with no batching\n",
    "\n",
    "So it's simplier to comment out the code doing that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk_embeddings = embedding_data(chunk_min_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we just added the embeddings to our dictionary on `chunk_min_token`\n",
    "\n",
    "We can actually batch them to make the process faster\n",
    "\n",
    "### Lets add the `text_chunk_embedding` inside our `chunk_min_token` dict\n",
    "\n",
    "✅than after we will save it to a csv to save our embedded format vector data in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now load our csv embeddings (tokens officially) into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_or_page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1. Checkpoints Inhibitors: Checkpoint inhibito...</td>\n",
       "      <td>878</td>\n",
       "      <td>128</td>\n",
       "      <td>219.5</td>\n",
       "      <td>[ 7.50207156e-02 -6.30471781e-02 -2.76404526e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Some drugs, known as apoptosis inducers, can t...</td>\n",
       "      <td>830</td>\n",
       "      <td>127</td>\n",
       "      <td>207.5</td>\n",
       "      <td>[ 2.01551374e-02 -1.88169186e-03 -3.07534169e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Senescence inducers target speciﬁc pathways in...</td>\n",
       "      <td>398</td>\n",
       "      <td>59</td>\n",
       "      <td>99.5</td>\n",
       "      <td>[ 4.52404656e-02  1.68271977e-02 -5.62751805e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   link_or_page_number                                     sentence_chunk  \\\n",
       "0                    0  1. Checkpoints Inhibitors: Checkpoint inhibito...   \n",
       "1                    0  Some drugs, known as apoptosis inducers, can t...   \n",
       "2                    0  Senescence inducers target speciﬁc pathways in...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               878               128              219.5   \n",
       "1               830               127              207.5   \n",
       "2               398                59               99.5   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [ 7.50207156e-02 -6.30471781e-02 -2.76404526e-...  \n",
       "1  [ 2.01551374e-02 -1.88169186e-03 -3.07534169e-...  \n",
       "2  [ 4.52404656e-02  1.68271977e-02 -5.62751805e-...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loaded chunks and tokens\n",
    "df_embeds = pd.read_csv('./chunks_tokenized.csv')\n",
    "df_embeds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have to convert the embeddings column in df into `tensors`\n",
    "\n",
    "First we have grab each element in embeddings row and make into a numpy array first before making them into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to np array on embeddings\n",
    "\n",
    "# Apply to each element in row (rows content) is what the apply(lamda) is doing. np already makes into array when you say it came from np.fromstring\n",
    "# fromstring into np array\n",
    "df_embeds[\"embeddings\"] = df_embeds[\"embeddings\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# convert df into list[dict]\n",
    "embeds_dict = df_embeds.to_dict(orient=\"records\")\n",
    "\n",
    "# convert into tensors\n",
    "embeddings = torch.tensor(np.array(df_embeds[\"embeddings\"].to_list()), dtype=torch.float32).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10077, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is standard dimension for the sentence transformer we got\n",
    "\n",
    "We also placed this embeddings into a `list` so we can use it later when it comes to processing the whole rag pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use the whole RAG this time to see if everything is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RAG\n",
    "# query = \"what treatments can be done when someone has breast cancer\"\n",
    "\n",
    "# # embed query\n",
    "# query_embeddings = embedding_model.encode(query,\n",
    "#                                           convert_to_tensor=True).to('cuda')\n",
    "\n",
    "# # compare query embeddings with data embeddings using dot score\n",
    "# start_time = timer()\n",
    "# dot_scores = util.dot_score(a=query_embeddings, b=embeddings)[0]\n",
    "# end_time = timer()\n",
    "\n",
    "# # Get top5 scores\n",
    "# scores, indices = torch.topk(dot_scores, k=5)\n",
    "\n",
    "# print(f\"Scores: {scores}\")\n",
    "# print(f\"Indices: {indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link_or_page_number': 13,\n",
       " 'sentence_chunk': 'Patients undergoing mastectomy may also need radiation if the tumor is large or there is lymph node involvement. Women undergoing mastectomy who elect breast reconstruction have several options, including the type of tissue or implant used to restore breast shape. Reconstruction may be performed at the time of mastectomy or later, but often requires more than one surgery. Depending on cancer stage, subtype, and sometimes other test results, such as tumor gene expression profiling (e.g., Oncotype DX), treatment may also involve chemotherapy (before and/or after surgery), hormone (anti-estrogen) therapy, targeted therapy, and/ or immunotherapy (e.g., immune checkpoint inhibitors). Survival: The 5- and 10-year relative survival rates are 91% and 85%, respectively, for invasive breast cancer overall, mostly because two-thirds of women are diagnosed with localized-stage disease. Despite progress over time, the 5-year survival rate is 10% lower (in absolute terms) for Black women (83%) than for White women (93%; Table 7), partly reflecting lower likelihood of localized-stage diagnosis (56% versus 67%). Reducing this and other cancer disparities is a focus of the American Cancer Society and many other organizations. See Breast Cancer Facts & Figures at cancer.org/statistics for more information on breast cancer. Cancer in Children and Adolescents New cases and deaths: In 2024, an estimated 9,620 children (ages 0 to 14 years) and 5,290 adolescents (ages 15-19 years) will be diagnosed with cancer, and 1,040 children and 550 adolescents will die from the disease. Cancer is the leading disease-related cause of death among both children and adolescents.',\n",
       " 'chunk_char_count': 1669,\n",
       " 'chunk_word_count': 248,\n",
       " 'chunk_token_count': 417.25}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_min_token[301]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make the visualization of the output more appealing by applying functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text wrapper\n",
    "\n",
    "def text_wrapper(text):\n",
    "    \"\"\"\n",
    "    Wraps the text that will pass here\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text = textwrap.fill(text, 80)\n",
    "\n",
    "    print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a function for the whole rag output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santi\\Deep-Learning\\rag\\medical-rag\\run-an-llm-from-scratch\\gemma\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Let's first get the embedding model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionize\n",
    "\n",
    "def rag_pipeline(query,\n",
    "                 embedding_model,\n",
    "                 embeddings,\n",
    "                 device: str,\n",
    "                 chunk_min_token: list):\n",
    "    \"\"\"\n",
    "    Grabs a query and retrieve data all in passages, augments them, than it\n",
    "    it outputs the top 5 relevant results regarding query's meaning using dot scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieval\n",
    "    query_embeddings = embedding_model.encode(query, convert_to_tensor=True).to(device)\n",
    "\n",
    "    # Augmentation\n",
    "    dot_scores = util.dot_score(a=query_embeddings, b=embeddings)[0]\n",
    "\n",
    "    # Output\n",
    "    scores, indices = torch.topk(dot_scores, k=5)\n",
    "    # counting = 0\n",
    "    # for score, idx in zip(scores, indices):\n",
    "    #     counting+=1\n",
    "    #     clean_score = score.item()*100\n",
    "    #     print(f\"For the ({counting}) result has a score: {round(clean_score, 2)}%\")\n",
    "    #     print(f\"On index: {idx}\")\n",
    "    #     print(f\"Relevant Text:\\n\")\n",
    "    #     print(f\"{text_wrapper(chunk_min_token[idx]['sentence_chunk'])}\\n\")\n",
    "\n",
    "    return scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapper(text, wrap_length=80):\n",
    "    \"\"\"Wraps any text input and prints it out\"\"\"\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function that creates the whole output on top results\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: List[Dict]=chunk_token_dict,\n",
    "                                 n_resources_to_return: int=5):\n",
    "   \"\"\"\n",
    "   Finds relevant passages given a query and print them out along with their scores\n",
    "   \"\"\"\n",
    "\n",
    "   scores, indices = rag_pipeline(query=query,\n",
    "                                  embedding_model=embedding_model,\n",
    "                                  embeddings=embeddings,\n",
    "                                  device='cuda',\n",
    "                                  chunk_min_token=chunk_min_token)\n",
    "   # Loop through zipped together scores and indices from torch.topk\n",
    "   # scores = scores.tolist()[0]\n",
    "   # indexes = indexes[1].tolist()[0]\n",
    "   for score, idx in zip(scores, indices):\n",
    "      print(f\"Score: {score:.4f}\")\n",
    "      print(\"Text:\")\n",
    "      print_wrapper(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "      print(f\"Page Number: {pages_and_chunks[idx]['link_or_page_number']}\")\n",
    "      print(f\"__Done__\\n\")\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the score rounded we can't see it as a tensor but rather python float which as simple as saying `score.item()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"How does a stroke happen?\"\n",
    "# rag_pipeline(query=query,\n",
    "#              embedding_model=embedding_model,\n",
    "#              embeddings=embeddings,\n",
    "#              device=\"cuda\",\n",
    "#              chunk_min_token=embeds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6482\n",
      "Text:\n",
      "Scoring Criteria for the DASH-Style Diet Score Table 5-2. This table shows the\n",
      "scoring criteria for the Dietary Approaches to Stop Hypertension, or DASH-style\n",
      "diet for the categories of fruits, vegetables, nuts and legumes, whole grains,\n",
      "low-fat dairy, sodium, red and processed meats, and sweetened beverages.\n",
      "Component Foods (NHANES 24-h recall) Scoring criteria Note Fruits All fruits and\n",
      "fruit juices Quintile 1: 1 point Quintile 2: 2 points Quintile 3: 3 points\n",
      "Quintile 4: 4 points Quintile 5: 5 points Higher score represents more ideal\n",
      "intake Quintile 1 is lowest consumption, and quintile 5 is highest consumption\n",
      "Vegetables All vegetables except potatoes and legumes Nuts and legumes Nuts and\n",
      "peanut butter, dried beans, peas, tofu Whole grains Brown rice, dark breads,\n",
      "cooked cereal, whole grain cereal, other grains, popcorn, wheat germ, bran Low-\n",
      "fat dairy Skim milk, yogurt, cottage cheese Sodium Sum of sodium content of all\n",
      "foods reported as consumed Quintile 1: 5 points Quintile 2: 4 points Quintile 3:\n",
      "3 points Quintile 4: 2 points Quintile 5: 1 point Reverse scoring in that higher\n",
      "quintiles represent less ideal intake Quintile 1 is lowest consumption, and\n",
      "quintile 5 is highest consumption Red and processed meats Beef, pork, lamb, deli\n",
      "meats, organ meats, hot dogs, bacon Sweetened beverages Carbonated and\n",
      "noncarbonated sweetened beverages The DASH diet score is assessed and points\n",
      "scored using the methods of Fung et al. 152 Quintiles of point score should be\n",
      "assigned using the most recent or most relevant NHANES data, appropriate to the\n",
      "question being addressed. DASH indicates Dietary Approaches to Stop\n",
      "Hypertension; and NHANES, National Health and Nutrition Examination Survey.\n",
      "Source: Reproduced with permission from Fung et al. 152 Copyright © 2008,\n",
      "American Medical Association. All rights reserved. Downloaded from\n",
      "http://ahajournals.org by on April 18, 2024\n",
      "Page Number: 96\n",
      "__Done__\n",
      "\n",
      "Score: 0.6408\n",
      "Text:\n",
      "34] J. D. Otvos, E. J. Jeyarajah, D. W. Bennett, and R. M. Krauss, “Development\n",
      "of a proton nuclear magnetic resonance spectroscopic method for determining\n",
      "plasma lipoprotein concentrations and subspecies distributions from a single,\n",
      "rapid measurement,” Clinical Chemistry, vol. 38, no. 9, pp. 1632–1638, 1992.\n",
      "[35] D. R. Witte, M. R. Taskinen, H. Perttunen-Nio, A. Van Tol, S. Livingstone,\n",
      "and H. M. Colhoun, “Study of 7 Oxidative Medicine and Cellular Longevity\n",
      "Page Number: 6\n",
      "__Done__\n",
      "\n",
      "Score: 0.6104\n",
      "Text:\n",
      "Merino J, Dashti HS, Sarnowski C, Lane JM, Todorov PV, Udler MS, Song Y, Wang H,\n",
      "Kim J, Tucker C, et al. Genetic analysis of dietary intake identi- fies new loci\n",
      "and functional links with metabolic traits. Nat Hum Behav. 2022;6:155–163.doi:\n",
      "10. 1038/s41562-021-01182-w 23. Sergeant S, Hugenschmidt CE, Rudock ME, Ziegler\n",
      "JT, Ivester P, Ainsworth HC, Vaidya D, Case LD, Langefeld CD, Freedman BI, et\n",
      "al. Differences in arachidonic acid levels and fatty acid desaturase (FADS) gene\n",
      "variants in African Americans and European Americans with dia- betes or the\n",
      "metabolic syndrome. Br J Nutr. 2012;107:547–555.doi: 10. 1017/S0007114511003230\n",
      "24.\n",
      "Page Number: 100\n",
      "__Done__\n",
      "\n",
      "Score: 0.5777\n",
      "Text:\n",
      "[8] R. M. Krauss, “Lipoprotein subfractions and cardiovascular disease risk,”\n",
      "Current Opinion in Lipidology, vol. 21, no. 4, pp. 305–311, 2010. [9] D.\n",
      "Steinberg, S. Parthasarathy, T. E. Carew, J. C. Khoo, and J. L. Witztum, “Beyond\n",
      "cholesterol. Modiﬁcations of low-density lipoprotein that increase its\n",
      "atherogenicity,” The New England Journal of Medicine, vol. 320, no. 14, pp.\n",
      "915–924, 1989. [10] A. N. Orekhov, V. V. Tertov, and D. N. Mukhin, “Desia-\n",
      "lylated low density lipoprotein—naturally occurring modi- ﬁed lipoprotein with\n",
      "atherogenic potency,” Atherosclerosis, vol.\n",
      "Page Number: 6\n",
      "__Done__\n",
      "\n",
      "Score: 0.5757\n",
      "Text:\n",
      "Author Manuscript Author Manuscript Author Manuscript Author Manuscript\n",
      "Page Number: 2\n",
      "__Done__\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual_query = \"How does cholestoral effect heart disease\"\n",
    "# retrieve_relevant_resources(query=actual_query,\n",
    "#                             embeddings=embeddings)\n",
    "print_top_results_and_scores(query=actual_query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link_or_page_number': 0,\n",
       " 'sentence_chunk': '1. Checkpoints Inhibitors: Checkpoint inhibitors are drugs that target molecules such as PD-1 or CTLA-4 on immune cells. These molecules act as brakes on the immune system, preventing it from attacking normal cells. By blocking these checkpoints, checkpoint inhibitors can \"release the brakes\" on the immune system, allowing it to recognize and attack cancer cells, weakening them. 2. Targeted Therapies: Targeted therapies are drugs that target speciﬁc molecules or pathways that are critical for cancer cell growth and survival. For example, tyrosine kinase inhibitors (TKIs) target enzymes called tyrosine kinases, which are involved in cell signaling pathways that promote cancer growth. By inhibiting these enzymes, targeted therapies can weaken cancer cells. 3. Apoptosis Inducers: Apoptosis is a process of programmed cell death that eliminates damaged or abnormal cells.',\n",
       " 'chunk_char_count': 878,\n",
       " 'chunk_word_count': 128,\n",
       " 'chunk_token_count': 219.5}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_min_token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great! We have made the RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionizing our semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input a query and you get a response from the augmentation that will be sent back to the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's put all of the steps from above for semantic search into a function, so we can repeat the workflow\n",
    "\n",
    "if this notebook wasn't here we'd just have a python file that has the helper functions, import pdf and does all text chunks, embedding and search step by step, that's the point of this project to make all into a function, so it's deployable for an app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting an LLM for local generation\n",
    "\n",
    "This process can also work with an LLM API\n",
    "\n",
    "LLM - Large Language Model\n",
    "\n",
    "LLMs are constantly being updated so experiment often with news one that be better with previous\n",
    "\n",
    "Since we want to run it locally we need to know how much compute power do we have, LLMs aren't small files so we should know this in advance. IF your looking for really big computation than you can use an API such as GPT-4 or Claude 3 but this has a trade off of waiting a response and having to pay that service to others, making you more dependable on them.\n",
    "\n",
    "a 7 billion LLM size model is a normal size, requires 28gb VRAM GPU on a float32 processing (which are more complete numbers on the data to find relations complete)\n",
    "\n",
    "7 billion parameter models, parameter here is the weight of the models, a small number that can learn relations and patterns on data. The more params the more the model has the opportunity to learn but the more compute cost since it has more weight to find relations which really on VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you increase the number of bytes or numbers to relate the more you `numeric precision`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer science, in precision it's how many numbers are used to calculate a number. for ex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1]).dtype, torch.tensor([1.0]).dtype\n",
    "# to get one it's represented in int64 which are sub numbers to represent that number of 1\n",
    "# for 1.0 it's float32 to represent that number, which is the datatype /as we know but there has to be smaller numbers\n",
    "# to represent that number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already get the data type on torch.tensor(), when half that precision we get less bytes so if we have 32 as 4 bytes, we half that 4 / 2 = 2 bytes and 16 bytes is 2 bytes, we than half it again, 2 / 2 = 1 byte which is grab by 8-bit for 1 bit and half that precision more, 1 / 2 = 0.5 bytes being 4-bit.\n",
    "\n",
    "Less and less bytes means less precisions and less reliance but less compute, you need to best the results through to what you can see can give you the right amount and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUT** this isn't a huge issue. Nowadays even having on a 4-bit vram, with the techiques done nowawadays gives pretty goo results, so it depends how you blend and best the results shown, the better is to blend a bit more bytes and still get a way better performance compared to cost what can be recieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is so many parameters, when you remove some of the bytes that represent them over such a large scale, the degregations aren't bad but they are noticable several times, so it depends on how you can utilize it and what your doing with the data.\n",
    "\n",
    "* The LLM you use is experimental and the data it takes up on your hardware will depend on the amount paraemters and percision you loaded it in, the performance of the model will depend on the amount of params it had as well as the precision that it computes in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "float 32 isn't very common as it's very very big but more common is float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good open source LLMs:\n",
    "\"These are contained normally 7 billion parameters\"\n",
    "* Hugging face\n",
    "* Gemma\n",
    "* Mistral AI\n",
    "* Llama 2\n",
    "\n",
    "Gemma seems right now to outperform by benchmark to other 2, so we'll use Gemma for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a big model make it smaller, accept the small performance degregation you're going to get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a model called The Bloke on hugging face, where it creates quantized (smaller version being a float16 Version and placing it to 4-bit VRAM) version of big models - https://huggingface.co/TheBloke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check local gpu computing memory\n",
    "\n",
    "To see which LLM you can choose that your GPU can support\n",
    "\n",
    "A Generative LLM\n",
    "\n",
    "Goes from text input ---> generate text output\n",
    "\n",
    "The LLM that you should use:\n",
    "\n",
    "* How much hardware VRAM do we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking our local GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our GPU contains 11 gb of memory\n"
     ]
    }
   ],
   "source": [
    "# Get GPU available memory\n",
    "import torch\n",
    "# Let's get memory in bytes\n",
    "# position 0 for gpu and get it's total memory\n",
    "# Our total memory in bytes\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "# Let's get the memory in gb\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30)) # 2 ** 30 = 1,073.7418 Million bytes = 1 gb\n",
    "print(f\"Our GPU contains {gpu_memory_gb} gb of memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However that isn't the amount of free space we just the total\n",
    "\n",
    "We should look at the free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 16:16:04 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.85                 Driver Version: 555.85         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti   WDDM  |   00000000:17:00.0  On |                  N/A |\n",
      "| 25%   33C    P8             32W /  250W |    2039MiB /  11264MiB |      2%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4468    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      5920    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A      6664    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A      7700    C+G   ...pdnekdrzrea0\\XboxGameBarSpotify.exe      N/A      |\n",
      "|    0   N/A  N/A      9044    C+G   ...1.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     10232    C+G   ...les\\Elgato\\CameraHub\\Camera Hub.exe      N/A      |\n",
      "|    0   N/A  N/A     12128    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     14196    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     18824    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19656    C+G   ...wekyb3d8bbwe\\XboxGameBarWidgets.exe      N/A      |\n",
      "|    0   N/A  N/A     20768    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     20852    C+G   ...tionsPlus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A     21828    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A     23660    C+G   ...1.0_x64__t4vj0pshhgkwm\\Telegram.exe      N/A      |\n",
      "|    0   N/A  N/A     23776    C+G   ...US\\ArmouryDevice\\asus_framework.exe      N/A      |\n",
      "|    0   N/A  N/A     27612    C+G   ...mpt_builder\\LogiAiPromptBuilder.exe      N/A      |\n",
      "|    0   N/A  N/A     27696    C+G   ...on\\124.0.2478.97\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     27904      C   ...rograms\\Python\\Python312\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     28832    C+G   ...ecurityApp\\MicrosoftSecurityApp.exe      N/A      |\n",
      "|    0   N/A  N/A     29836    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     30316    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     30496    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     32280    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used 7.8gb of VRAM on the gpu which makes remaining, 2.8 gb we can use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To download models and run locally from hugging face, you will need to sign in to hugging face CLI\n",
    "\n",
    "**Do not enter `huggingface-cli login` inside the console**, do it in a seperate terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IT is instruction tune, meaning the base language model has been fine tuned to follow instructions.\n",
    "\n",
    "Which is make me this and that, it will show you text and and value listed down, that's instruction tuning, similiar to the terminal, where you give it commands and responds to them. THis is good to access more direct and custom work to flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find our gpu memory that can work with gemma size models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 11 | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\n",
      "use_quantization_config set to: False\n",
      "model_id set to: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an LLM locally With Gemma 2B in float 16 precision\n",
    "\n",
    "We can load an LLM locally using Hugging face `transformers`.\n",
    "\n",
    "The model we'll use on Nvidia 2080 Ti is Gemma 2B float16 VRAM\n",
    "\n",
    "\"*Less bits, the less space it will take up.*\"\n",
    "To get a model running locally we'll need several things:\n",
    "1. A quantization config (optional) - a config on what precision to load the model in (e.g. 8bit, 4bit, etc)\n",
    "2. A model id - this will give transformer which model/tokenizer to load (the type of tokenizer being used)\n",
    "3. A tokenizer - this turns text into numbers to read for the LLM (NOTE: a tokenizer is different from an embedding model, a tokenizer will be specific to your LLM) The model id, hugging face makes it simple for transformers to pair the tokenizer and model you used\n",
    "4. An LLM model - this will be what we use to generate text based on an input\n",
    "\n",
    "> **NOTE:** There are many tips and tricks on loading/making LLMs work faster. One of the best ones is flash_attn (Flash Attention 2.) View they're github for more info - https://github.com/Dao-AILab/flash-attention\n",
    ">> Flash attention is a self attention mechanism where it gets the input sequence tokens and groups them up into several groups to focus on attention on each group with several tokens rather than each token at a time which would increase computational cost, The Flash Attention provides a lower computation which would make it for more efficient on long text queries so the model can process. **The only problem:** is that this is right now 04/2024 only available for more recent bigger GPU such as Ampere, Ada, Hopper GPUS (e.g., A100, RTX 3090, RTX 4090, H100). Support for Turing GPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing GPUs for now than flash attention 2\n",
    ">> You can view your compute capability here - https://developer.nvidia.com/cuda-gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are not going to use quantization config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `is_flash_attn_available` is deprecated and will be removed in v4.38. Please use `is_flash_attn_2_available` instead.\n",
      "c:\\Users\\santi\\Deep-Learning\\rag\\medical-rag\\run-an-llm-from-scratch\\gemma\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db4d2d48133401d940237870117e05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "# Auto Tokenizer will create or tokizer automatically and AutoMOdelForCausualLM is to create a auto LM which is causal\n",
    "# Casual is also a gnerative LM, based on the models ability to generate output based on early input sequences,\n",
    "# such as the next token is generated based on the input tokens for before, it relies entirely on the past tokens sequence\n",
    "# and action to make the next token, they must carry a chain link\n",
    "\n",
    "# The Causal model predicts the next token in a sequence based on the previous tokens in the sequence during training and generation\n",
    "# This process is a self attention mechanism, where next tokens are generated on links and relations on previous tokens sequences\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_available \n",
    "\n",
    "# 1. Create a quantization config\n",
    "# NOTE: requires !pip install bitsandbytes accelerate\n",
    " # Develops better on bits and bytes feature\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, # load model in 4-bit\n",
    "                                         bnb_4bit_compute_dtype=torch.float16) # compute in float 16 precision, a bit weird but it seems to work, find out more with your research\n",
    "\n",
    "#  Bonus: flash attention 1 = faster attention mechanism\n",
    "#  Flash Attention 2 requires a GPU with a compute capabilitiy score of 8.0+ as explained on markdown above\n",
    "# Our GPU compute score is 7.5 as Nvidia RTX 2080 Ti\n",
    "if (is_flash_attn_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_1\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\" # scaled dot product attention, it's fast because on pytorch they have used\n",
    "    # scaled dot product attention for flashAttention-v2 but it works for flash_attention_1\n",
    "\n",
    "# 2. Pick model we should use (Gemma 2B-it)\n",
    "model_id = model_id\n",
    "\n",
    "# 3. Instantiate tokenizer (turns text into tokens)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# Instantiate the model\n",
    "# there is a lot more parameters in here that you can try and tweak so try them out with several other biases\n",
    "# to get more direct and wanted model for your project\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 quantization_config=quantization_config,\n",
    "                                                 low_cpu_mem_usage=False, # Use as much memory as we can\n",
    "                                                 attn_implementation=attn_implementation)\n",
    "\n",
    "# IF we have set a config, it will take care of the device placement automatically, that's part of the biteandbytes config\n",
    "# and huggings face accelerate\n",
    "\n",
    "# if we're not using this config than send the model (LLM model) to the GPU\n",
    "# if not use_quantization_config:\n",
    "#     llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using attention implementation: sdpa\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using attention implementation: {attn_implementation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you load a big model and don't have enough GPU space it will offload it to CPU but we want to use as much memory for our GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your downloading for the first time gemma, it would take awhile to load. Depending on internet connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability(0) # our device capability can be found on torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 16:16:35 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.85                 Driver Version: 555.85         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti   WDDM  |   00000000:17:00.0  On |                  N/A |\n",
      "| 25%   33C    P8             32W /  250W |    3700MiB /  11264MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4468    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      5920    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A      6664    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A      7700    C+G   ...pdnekdrzrea0\\XboxGameBarSpotify.exe      N/A      |\n",
      "|    0   N/A  N/A      9044    C+G   ...1.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     10232    C+G   ...les\\Elgato\\CameraHub\\Camera Hub.exe      N/A      |\n",
      "|    0   N/A  N/A     12128    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     14196    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     18824    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19656    C+G   ...wekyb3d8bbwe\\XboxGameBarWidgets.exe      N/A      |\n",
      "|    0   N/A  N/A     20768    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     20852    C+G   ...tionsPlus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A     21828    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A     23660    C+G   ...1.0_x64__t4vj0pshhgkwm\\Telegram.exe      N/A      |\n",
      "|    0   N/A  N/A     23776    C+G   ...US\\ArmouryDevice\\asus_framework.exe      N/A      |\n",
      "|    0   N/A  N/A     27612    C+G   ...mpt_builder\\LogiAiPromptBuilder.exe      N/A      |\n",
      "|    0   N/A  N/A     27696    C+G   ...on\\124.0.2478.97\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     27904      C   ...rograms\\Python\\Python312\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     28832    C+G   ...ecurityApp\\MicrosoftSecurityApp.exe      N/A      |\n",
      "|    0   N/A  N/A     29836    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     30316    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     30496    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     32280    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Let's check our remaining space of GPU memory\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1515268096"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    # Sum the number of params in model parameters\n",
    "    # Sum the whole thing on different elements using a list []\n",
    "    # amount of elements in params being every params themselves to get in param.num() as numbers in total\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has 1.5 billion parameters in the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 2106740736, 'model_mem_mb': 2009.14, 'model_mem_gb': 1.96}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How you can get the memory numbers for the different gemma sizes (2B, 7B, 10B) etc\n",
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    # Get model parameters and buffer sizes\n",
    "    # nelement() is the number of elements in the parameter\n",
    "    #element_size() returns the numbers amount of elements contained in parameter which is dimensions and amount\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    # Let's do the same for the memoru buffers\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate model sizes\n",
    "    # how much the model is taking up memory in our device\n",
    "    model_mem_bytes = mem_params + mem_buffers\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # conversion to megabytes with bytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # conversion to gigabytes with bytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes, \n",
    "            \"model_mem_mb\": round(model_mem_mb, 2), \n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the size of our model\n",
    " \n",
    "To load Gemma 2B float16 we need a minimum of 4.73 gb of VRAM on our GPU. However, due to the calculations that take place in the forward pass (which is when we pass input text to our model to process), we need closer to 8 gb\n",
    "\n",
    "Loading our LLM model is one thing which takes up space but to have model process input text takes up more memory, in my case it's 8gb which we don't have\n",
    "\n",
    "Even fitting a v100 gpu to a Gemma 7B-it on float16 would barely just fit for the LLM model and can't process as storage the input text that's will pass in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text with our LLM\n",
    "\n",
    "* Note: some models have been trained/tuned to generate text with a specific template in mind\n",
    "\n",
    "Because `gemma-7b-it` has been trained in an instruction-tuned manner, we should follow the instruction template for the best results, similiar to when using chat gpt you don't have to format your text in a certain way for the model to understand since they fine tuned it to understand it that way. So the training process is what determines it's bias to understand text through a model, It's best if we get a model in the future (an LLM model) that isn't limited to some command intructions like which isn't scalable to retrieve information within it's data and augmentation rather we should have a model to understand general complex human text than some robot command.\n",
    "\n",
    "We'll do that for another project, for now since we're using this the first time let's just follow the same template gemma was built on\n",
    "\n",
    "Though the instructions are general, it doesn't do well with general feedback on questions and related depth about relations on what was discussed to understand what's being said more. instructions are more do this and that or when this rather if you ask things the amount of x goes into y, why was there another point, you said in z it effects that point, it can move to another area.. The model here wouldn't understand well because your giving it multi amounts of questions and relations not at least relevant now but will in the future regarding these more human and understand process of questions, to model the human not the machine as just intstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text on LLM with local\n",
    "# input_text = \"What are the macronutrients, and what roles do they play in the human body\"\n",
    "#  This will generation without retrieval \n",
    "# Our model is trained on internet text but it's recieving a retrieval from our data base through on dialogue as\n",
    "# natural langure to understand something, rather the database can be the direct answer it grabs, but the generation\n",
    "# on text to give it tell you this process will need this, wiht what we discussed it would need this and because your looking\n",
    "# to advance that other part these other nutrients would provide more effets that can lead to a effect on making what\n",
    "# you want so try those parts.. This sort of dialogue which is factual informative as it grabbed in ourcase a pbunch of\n",
    "# the fact data it retreived on query and found relevant relations to it's passages and outputted it out, the large language model\n",
    "# is what's responsible of wrapping the augmentation on what it found relation to the text to know use it's understand\n",
    "# more of human explanation and logically forward thinking a question as they know intuitively that if one plays on another thing\n",
    "# than another it would lead to a result similiar. MOstly the LLM wraps the text isntead of just pasting straight what i grabbed\n",
    "# from the textbook, but really wrap these facts with explain your text can help with function, these effects lead to another part\n",
    "# it may be scalable, you can view this it will help on more relieve and can generate a place portion on another text. \n",
    "# TO understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's format our input text into a tokenized prompt template for LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make the prompt to ask the LLM model directly a question without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santi\\Deep-Learning\\rag\\medical-rag\\run-an-llm-from-scratch\\gemma\\venv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:561: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      "<bos><bos><start_of_turn>user\n",
      "how does cancer grow?, keep your explanation complete, i don't want sentences that weren't completed.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Cancer cells are abnormal cells that have the ability to grow and spread to other parts of the body. They do this by:\n",
      "\n",
      "**1. Mutation:** Cancer cells have an increased ability to mutate, meaning they can change their behavior and become more aggressive. This can be caused by various factors, including exposure to carcinogens (cancer-causing substances), genetic mutations, and certain environmental factors.\n",
      "\n",
      "**2. Cell cycle errors:** Cancer cells often have genetic mutations that allow them to bypass normal cell cycle controls, leading to uncontrolled cell growth and division.\n",
      "\n",
      "**3. Angiogenesis:** Cancer cells can form blood vessels to grow and spread to other parts of the body. This is essential for cancer cells to receive nutrients and oxygen.\n",
      "\n",
      "**4. Invasion and metastasis:** Cancer cells can invade surrounding tissues and spread to other parts of the body through a process called metastasis. This can occur through the lymphatic system, bloodstream, or direct spread through blood vessels.\n",
      "\n",
      "**5. Hormonal effects:** Some cancer cells can produce hormones that promote their growth and survival.\n",
      "\n",
      "**6. Immune response evasion:** Cancer cells can avoid detection by the immune system by expressing proteins that suppress immune responses.\n",
      "\n",
      "**7. Tissue remodeling:** Cancer cells can remodel the surrounding tissue environment to create a more\n",
      "\n",
      "CPU times: total: 12.2 s\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = input(\"\")\n",
    "\n",
    "def gemma(input_text: str,\n",
    "          tokenize=False,\n",
    "          add_generation_prompt=True,\n",
    "          max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    Gemma-2b-it LLM model, already contains tokenization and conversion process. All you pass is the question.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reformat prompt into technique\n",
    "    input_text += \", keep your explanation complete, i don't want sentences that weren't completed.\"\n",
    "    \n",
    "    # Template\n",
    "    dialogue_template = [{\"role\": \"user\",\n",
    "                          \"content\": input_text}]\n",
    "    \n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                           tokenize=tokenize,\n",
    "                                           add_generation_prompt=add_generation_prompt) # generation prompt\n",
    "\n",
    "    # TOKENIZATION\n",
    "    input_ids = tokenizer(prompt,\n",
    "                      return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # llM generate and ouput with input_ids\n",
    "    output_token = llm_model.generate(**input_ids,\n",
    "                                      max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Convert ouput_token into texy\n",
    "    output_text = tokenizer.decode(output_token[0])\n",
    "\n",
    "    print(f\"Model output (decoded):\\n{output_text}\\n\")\n",
    "\n",
    "gemma(input_text=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      " How does cancer cells mutate?\n",
      "\n",
      " Prompt (formatted):\n",
      "<bos><start_of_turn>user\n",
      "How does cancer cells mutate?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"How does cancer cells mutate?\"\n",
    "print(f\"Input text:\\n {input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "# The template the model will recieve as input\n",
    "dialogue_template = [{\"role\": \"user\",\n",
    "                      \"content\": input_text}]\n",
    "# In the tokenizer, it's able to convert dialogue_template we instiated into the actual right template (numerical for\n",
    "# for our large language model) automatically\n",
    "\n",
    "# Apply the chat template\n",
    "# this will convert our dictionary with role and content into a list of tokens\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True) # generation prompt\n",
    "print(f\"\\n Prompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of turn for user is the person asking the question on prompt and start of turn model now it's there place to grab that text on prompt and process into the Generative LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Prompt` is the of the input given to a generative LLM\n",
    "\n",
    "Prompt engineering is to structure a text-based input to a generative LLM in a specific way so the generative output is ideal for it's user on prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad, end of sentence, beginning of sentence, unknown, start of turn, end of turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,      2,    106,   1645,    108,   2299,   1721,   9198,   5999,\n",
       "         167956, 235336,    107,    108,    106,   2516,    108]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Our input text was the input_text but we formatted it with the tokenizer as the right template for the prompt\n",
    "# But we have't fully tokenized this template yet which we need to do, all the tokenizer did was move and organize\n",
    "# the text and also instantiate a program that has it read to move from user into now the model, which the tokenization proces\n",
    "# will find and tokenize simply this whole template into the exact same instruction but in a numerical format\n",
    "\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to the GPU\n",
    "input_ids = tokenizer(prompt,\n",
    "                      return_tensors=\"pt\").to(\"cuda\") # pt stands for pytorch, return tensors through pytorch and send it to the gpu, to compute\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these tensors are visible because there attention mask is 1 but you can mask out some the inputs via attention mask by setting several tensors in attention_mark to 0 but in our case we want to view the tensors completly, to then send it too the LLM model\n",
    "\n",
    "We want our model to generate tokens based on the tensors on input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An LLM outputs tokens **NOT** Text\n",
    "\n",
    "We have to convert it to texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given input_id tokens from above, generate a maximum amount of 256 more\n",
    "\n",
    "So we limited the amount of new tokens we can get, this likely is for efficiency on computing power for process in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the model find relations to our question in tensors to..\n",
    "\n",
    "it's own training data and params to find relations between it's training and our tokenized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output tokens: tensor([     2,      2,    106,   1645,    108,   2299,   1721,   9198,   5999,\n",
      "        167956, 235336,    107,    108,    106,   2516,    108,    688,  60013,\n",
      "          5999,    791,   4282,  22154,    577, 167956, 235269,   3359,  66058,\n",
      "           109,    688, 235274, 235265,  14349,   5855, 185261,  66058,    109,\n",
      "        235287,   5231,  23281,  43009,  66058,   3766,   4559,    575,    573,\n",
      "         11999,  10629,    798,  10396,   8700,   6887, 235269,   8133,    577,\n",
      "          9198,   3505, 235265,    108, 235287,   5231,   7376,   1758,  55916,\n",
      "         66058,   4213,   9198,   5999,  27033,   3437,  17058,    576,   3724,\n",
      "         70753, 235269,   2183,   3588,  10225,  17058, 235265,    108, 235287,\n",
      "          5231,  40048,    576,  24147, 156997,  19774,  66058, 185261,    575,\n",
      "         24147, 156997,  19774,    798,  58473,   3027,  10501,  19084, 235269,\n",
      "          8133,    577, 120669,   3027,   5115, 235265,    109,    688, 235284,\n",
      "        235265, 197802, 228638,  66058,    109, 235287,   5231,  23807,   7042,\n",
      "         66058,  43961,    577,  93878, 108272,    689,   1156,  11999, 235290,\n",
      "         74900,   3360,  13866,    798,   4557,  11999,   7042, 235269,   8133,\n",
      "           577,  43009, 235265,    108, 235287,   5231,  12089, 198860, 108064,\n",
      "         66058,  13763,  10072,    484,    708,  21526,  22515,    696,    573,\n",
      "         11338,    576,  70753, 235265,   3194,   8175,  10072,    484,   3831,\n",
      "          1980,   3309, 235269,    573,   5999,    798,    793,   5543,  24471,\n",
      "           578,  38442,   3027,  10501,  19787, 235265,    109,    688, 235304,\n",
      "        235265, 137414, 115892,  36844,    903,  66058,    109, 235287,   5231,\n",
      "         58023,    576,  99466,  11691,  66058, 185261,    575,  19774,   6664,\n",
      "           575,   3027,   5115, 235269,  19034, 235269,    578,  39885,    798,\n",
      "         14650,   9198,   3505, 235265,    108, 235287,   5231,  75384,    576,\n",
      "          8220, 235290,    477,    528,  11691,  66058,  74945,    576,   8220,\n",
      "        235290,    477,    528,  11691,    798,   2389,    577,    573,   6887,\n",
      "           576,   9198, 235290,  10451,  19774, 235265,    109,    688, 235310,\n",
      "        235265,  19943,    602,  57801,  36844,    903,  66058,    109, 235287,\n",
      "          5231,  23807, 107241,  66058, 154592, 107241,  12136,    798,  10396,\n",
      "          8700,   6887,   2346,   4559,    575,    573,  11999,  10629, 235265,\n",
      "           108, 235287], device='cuda:0')\n",
      "\n",
      "CPU times: total: 13.3 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate outputs from local LLM\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256)\n",
    "\n",
    "print(f\"Model output tokens: {outputs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our models did tokens in and tokens out**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We placed input template in tokens, and the LLM response to tokens relations has outputted the response to input tokens in a token format but..\n",
    "\n",
    "We want in back in text for our user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's decode the tokens our model generated as output into as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      "<bos><bos><start_of_turn>user\n",
      "How does cancer cells mutate?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "**Cancer cells have various mechanisms to mutate, including:**\n",
      "\n",
      "**1. Somatic Mutations:**\n",
      "\n",
      "* **Gene mutations:** These changes in the DNA sequence can alter gene expression, leading to cancer development.\n",
      "* **Copy number alterations:** Some cancer cells acquire extra copies of specific chromosomes, while others lose copies.\n",
      "* **Loss of tumor suppressor genes:** Mutations in tumor suppressor genes can disrupt cell cycle regulation, leading to uncontrolled cell growth.\n",
      "\n",
      "**2. Genomic Instability:**\n",
      "\n",
      "* **DNA damage:** Exposure to carcinogens or other DNA-damaging agents can cause DNA damage, leading to mutations.\n",
      "* **Telomere shortening:** Telomeres are protective caps at the ends of chromosomes. When telomeres become too short, the cells can no longer divide and undergo cell cycle arrest.\n",
      "\n",
      "**3. Signaling Pathway Alterations:**\n",
      "\n",
      "* **Mutation of oncogenes:** Mutations in genes involved in cell growth, survival, and differentiation can promote cancer development.\n",
      "* **Activation of proto-oncogenes:** Activation of proto-oncogenes can lead to the expression of cancer-related genes.\n",
      "\n",
      "**4. Epigenetic Alterations:**\n",
      "\n",
      "* **DNA methylation:** Abnormal methylation patterns can alter gene expression without changes in the DNA sequence.\n",
      "*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the output tokens to text\n",
    "# instead of encoding in our tokenizer for text to token, now we need token to text through decoding it with the\n",
    "# tokenizer class\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part where it says \"Sure, here's a breakdown of the macronutrients and their roles in the human body:\n",
    "\" that's the Generative LLM processing that gives it outputs relative to a human text having several explanation on with an algorithm to than make after key points to bring to macronutrients nad the key points relating more to macronutrient, proteins and fat. The LLM actually doesn't output flat what it got on data with our data in textbook, it saw it but processed it in the LLM with input to say these are the main points and that plauys a role, sort of like explaining to more understandable explanation and does more to even highlt and do the same with a someinsight though not asked but that could be relevant more with the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just made our GEN LLM model but isn't connect to our RAG pipeline\n",
    "\n",
    "let's fix that\n",
    "\n",
    "by first..\n",
    "\n",
    "### Make a list of input text for local LLM and GPT3.5 for questions\n",
    "\n",
    "With combining the text together, let's feed that query text to our RAG augmentation to our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do specific genetic mutations contribute to the aggressiveness of certain types of cancer?',\n",
       " 'What is the relationship between stroke-induced brain damage and the subsequent development of cardiovascular diseases?',\n",
       " 'What are the emerging targeted therapies for treating triple-negative breast cancer, and how do they compare to traditional treatments?',\n",
       " 'What role does fibre play in digestion? Name five fibre containing foods.',\n",
       " 'How does the long-term impact of COVID-19 on the cardiovascular system differ from that of other viral infections',\n",
       " 'How does genetics effect cancer?',\n",
       " 'Can you get heart disease if you had a stroke?',\n",
       " 'What type of treatment should you do if you get breast cancer?',\n",
       " 'How do long-term effects of COVID-19 impact cardiovascular health?']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nutrition-style questions generated with GPT4\n",
    "gpt4_questions = [\n",
    "    \"How do specific genetic mutations contribute to the aggressiveness of certain types of cancer?\",\n",
    "    \"What is the relationship between stroke-induced brain damage and the subsequent development of cardiovascular diseases?\",\n",
    "    \"What are the emerging targeted therapies for treating triple-negative breast cancer, and how do they compare to traditional treatments?\",\n",
    "    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
    "    \"How does the long-term impact of COVID-19 on the cardiovascular system differ from that of other viral infections\"\n",
    "]\n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"How does genetics effect cancer?\",\n",
    "    \"Can you get heart disease if you had a stroke?\",\n",
    "    \"What type of treatment should you do if you get breast cancer?\",\n",
    "    \"How do long-term effects of COVID-19 impact cardiovascular health?\"\n",
    "]\n",
    "\n",
    "\n",
    "query_list = gpt4_questions + manual_questions\n",
    "query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our generation step, tokenization and embeddment ready we just need to make sure:\n",
    "1. Our augmentation embeddment foramt is the same format to the LLM tokenization or vice versa\n",
    "2. We need to combine our augmentation process of query with passages on RAG and LLM to finalize an output for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the relationship between stroke-induced brain damage and the subsequent development of cardiovascular diseases?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rag_pipeline() missing 3 required positional arguments: 'embedding_model', 'device', and 'chunk_min_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Get just the scores and indices of top related \u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m scores, indices \u001b[38;5;241m=\u001b[39m \u001b[43mrag_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                               \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m scores, indices\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# We need relevant passages to be with the query before even the embed process on RAG\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: rag_pipeline() missing 3 required positional arguments: 'embedding_model', 'device', and 'chunk_min_token'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get just the scores and indices of top related \n",
    "scores, indices = rag_pipeline(query=query,\n",
    "                               embeddings=embeddings)\n",
    "\n",
    "scores, indices\n",
    "\n",
    "# We need relevant passages to be with the query before even the embed process on RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have just retreived and generated, now let's pass it our LLM model\n",
    "\n",
    "Augmenting our prompt with context items\n",
    "\n",
    "We've done retreival.\n",
    "\n",
    "We've done generation\n",
    "\n",
    "let's augment now!\n",
    "\n",
    "The concept of augmenting a prompt with context items is also referred as prompt engineering.\n",
    "\n",
    "(Best way to put inputs in a LLM and get outputs) RAG is best for specific fact needance for prompt engineering\n",
    "\n",
    "Prompt engineering is an active field of research and several styles will come to be show as new in all\n",
    "\n",
    "However, there are a fair few techniques that work quite well.\n",
    "\n",
    "Resources:\n",
    "* https://github.com/brexhq/prompt-engineering?tab=readme-ov-file#what-is-a-large-language-model-llm\n",
    "* https://arxiv.org/abs/2401.14423\n",
    "* https://www.promptingguide.ai/\n",
    "* https://www.anthropic.com/news/prompt-engineering-for-business-performance\n",
    "\n",
    "We're going to use a several techniques to tell to LLM\n",
    "(\"Given this input, i want the output to look this this.\"), so it can understand what next thing more to do\n",
    "1. Give clear instructions\n",
    "2. Give a few examples of input/output (e.g. given this input, I'd like this output).\n",
    "3. Give room to think (e.g. create a scratchpad/\"show your working space\"/\"let's think step by step..\") This part actually works with LLMs, you need to show that it's working in itself to understand, if it get's here than there to focus more in what you seem than give working\n",
    "on step by step since the model saturates it search through the input, if you narrow it more on telling it things it will do that part your asking more with what you said before so it narrows it down and show working space has it know how it got from there in all with your input and you can ask it questions and advice on search to search better on what it sees to better respond to more based on what your saying and to th next thing you could say which is somewhat good but its better to give someone direct results and they will want to work with you in more for all.\n",
    "\n",
    "\n",
    "Let's create a function to format a prompt with context items.\n",
    "\n",
    "Let's give the text input as instructions for input in a clear way, than create to folow it's research completely from the response grabbed from the augmentation of the RAG so it can format in it's explanatory model the facts from docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to format prompt with context items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link_or_page_number': 48,\n",
       " 'sentence_chunk': 'Eur J Med Res. Mar 26 2022;27(1):47.doi:10. 1186/s40001-022-00677-0. 72. Kruszon-Moran D, Paulose-Ram R, Martin CB, Barker LK, McQuillan G. Prevalence and Trends in Hepatitis B Virus Infection in the United States, 2015-2018. NCHS Data Brief. Mar 2020;(361):1-8. 73. Danta M, Rodger AJ. Transmission of HCV in HIV-positive populations.',\n",
       " 'chunk_char_count': 335,\n",
       " 'chunk_word_count': 48,\n",
       " 'chunk_token_count': 83.75}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_token_dict[420]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This will be the prompt we will give model.\n",
    "\n",
    "to process the augmentation output results and view the query from user, it should format it's explanation by looking at the documents, from augmentation and give a depth thourough answer to something specific\n",
    "prompt example:\n",
    "\n",
    "based on the following context :\n",
    "- aosdjaosd\n",
    "- woiaooiaoiw\n",
    "- wamkcmkxm\n",
    "- pawoqoa\n",
    "- samdsopa\n",
    "\n",
    "\n",
    "Please answer the following  query: What are the macronutrients and what do they do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's format the prompt, we haven't passed to the LLM but will eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # context items are the relevant data retrieved from previous interactions of prompts or topic being asked by the\n",
    "# # prompt that the LLM structure can process and give a solution based on that context item topic spotting from prompt\n",
    "# def prompt_formatter(query: str,\n",
    "#                      context_items: List[Dict]) -> str:\n",
    "#     # context items will be from pages and chunks, the actual specific data topic\n",
    "#     context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "#     prompt = context\n",
    "#     # Let's join them as dot points, Based on these context .... Generate thing\n",
    "\n",
    "#     return prompt # the prompt is just the context\n",
    "    \n",
    "# def RAG_context_prompt(query: str, embeddings):\n",
    "#     \"\"\"Returns a random query and outputs the context items respected to the query\"\"\"\n",
    "\n",
    "#     print(f\"Query: {query}\")\n",
    "\n",
    "#     # Get relevant resources\n",
    "#     scores, indices = retrieve_relevant_resources(query, embeddings)\n",
    "\n",
    "#     # Create a list of context items\n",
    "#     # each item in pages_and_chunks\n",
    "#     context_items = [pages_and_chunks[i] for i in indices]\n",
    "#     # This the augmentation it should want to view being related towards queries question, The LLM shoudl use this\n",
    "#     # as a resources to further answer the queries question with more depth on specific material, without general steps\n",
    "#     # to do something but rather direct ones\n",
    "\n",
    "#     # Format our prompt\n",
    "#     prompt = prompt_formatter(query=query, context_items=context_items)\n",
    "\n",
    "#     # return prompt\n",
    "#     print(prompt)\n",
    "\n",
    "# query = random.choice(query_list)\n",
    "# RAG_context_prompt(query, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our base_prompt is the instructions we will give to LLM to use our RAG, it grab the RAGS ourput related to query, than LLM will response to query regarding the sources it was sent through rag to relevantly respond thorough to the users question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does the long-term impact of COVID-19 on the cardiovascular system differ from that of other viral infections\n",
      "<bos><start_of_turn>user\n",
      "In this text, you will act as supportive medical assistant.\n",
      "Give yourself room to think.\n",
      "Explain each topic with facts and also suggestions based on the users needs.\n",
      "Keep your answers thorough but practical.\n",
      "You as the assistant will recieve context items for retrieving information.\n",
      "Use the following examples as reference for the ideal answer style.\n",
      "\n",
      "Example 1. \n",
      "Query: How does genetics effect cancer?\n",
      "Answer: Some people inherit gene mutations (changes) from their parents. These inherited mutations are though rare but can occur in BRCA1 and BRCA2 gene mutations, which are related to break or ovarian cancer.\n",
      "\n",
      "Example 2.\n",
      "Query: Can you get heart disease if you had a stroke?\n",
      "Answer: Yes, you can get heart disease. Chances get higher if you are viewing increase in blood pressure, diabites, cholesterol or smoking. Strokes however don't directly cause heart disease but does lead to it if no treatment is done.\n",
      "\n",
      "Example 3.\n",
      "Query: What type of treatment should you do if you get breast cancer?\n",
      "Answer: That may vary depending on the size of the cancer? If you got a response from your doctor I can help suggest you what considerations you can do. One of the main treatments you can do is getting a surgey, chemotherapy or hormonal therapy. If you give me more details on your doctors response, I can help narrow what treatment can work best according to your record and body. \n",
      "\n",
      "Now use the following context items to answer the user query:\n",
      "- Int. J. Mol. Sci. 2023, 24, 4321 18 of 31 Table 1. Cont. Study Study Design Key Findings Amore L et al.,2022 [244] Prospective cohort study; 15 patients with dilated cardiomyopathy and reduced LVEF; administration of sacubitril/valsartan for 6 months. Administration of sacubitril/valsartan shows an increase in reactive hyperemia index and AIx six months after ﬁrst administration. Endothelin receptor antagonists (ERAs) Karavolias GK et al.,2010 [256] Cross-sectional study; 16 patients with moderate-severe idiopathic PAH under conventional treatment; administration of bosentan (62. 5 mg twice daily for 1 month followed by 125 mg twice daily for 11 months).\n",
      "- Atherosclerosis 2009, 207, 144–149. [CrossRef] [PubMed] 67. Kubo, M.; Hata, J.; Ninomiya, T.; Matsuda, K.; Yonemoto, K.; Nakano, T.; Matsushita, T.; Yamazaki, K.; Ohnishi, Y.; Saito, S.; et al. A nonsynonymous SNP in PRKCH (protein kinase C η) increases the risk of cerebral infarction. Nat. Genet. 2007, 39, 212–217. [CrossRef] [PubMed] 68. Lee, T.; Ko, T.; Chen, C.; Lee, M. M.; Chang, Y.; Chang, C.; Huang, K.; Chang, T.; Lee, J.; Chang, K.; et al. Identiﬁcation of PTCSC3 as a Novel Locus for Large-Vessel Ischemic Stroke: A Genome-Wide Association Study.\n",
      "- doi: 10. 1016/j.ajogmf. 2019. 100053 105. Urquia ML, Wall-Wieler E, Ruth CA, Liu X, Roos LL. Revisiting the association between maternal and offspring preterm birth using a sibling design. BMC Pregnancy Childbirth. 2019;19:157.doi: 10. 1186/s12884-019-2304-9 106. Wadon M, Modi N, Wong HS, Thapar A, O’Donovan MC. Recent advances in the genetics of preterm birth. Ann Hum Genet. 2020;84:205–213.\n",
      "- 0123456789();: 71. Hou, Y. J. et al. SARS- CoV-2 reverse genetics reveals a variable infection gradient in the respiratory tract. Cell 182, 429–446 (2020). This study shows that SARS- CoV-2 infection levels in COVID-19 autopsied lungs corresponds to a gradient of ACE2 expression in the upper and lower airways. 72. Sungnak, W. et al. SARS- CoV-2 entry factors are highly expressed in nasal epithelial cells together with innate immune genes. Nat. Med.\n",
      "- J. Card. Fail. 2021, 27, 387–413. [CrossRef] 2. Becher, P. M.; Lund, L. H.; Coats, A. J. S.; Savarese, G. An update on global epidemiology in heart failure. Eur. Heart J. 2022, 43, 3005–3007. [CrossRef] [PubMed] 3. Ho, J. E.; Lyass, A.; Lee, D. S.; Vasan, R. S.; Kannel, W. B.; Larson, M. G.; Levy, D. Predictors of new-onset heart failure: Differences in preserved versus reduced ejection fraction. Circ.\n",
      "\n",
      "Relevant passages: Please extract the context items that helped you answer the user's question\n",
      "<extract relevant passages from the context here>\n",
      "User query: How does the long-term impact of COVID-19 on the cardiovascular system differ from that of other viral infections\n",
      "Answer:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# context items are the relevant data retrieved from previous interactions of prompts or topic being asked by the\n",
    "# prompt that the LLM structure can process and give a solution based on that context item topic spotting from prompt\n",
    "def prompt_formatter(query: str,\n",
    "                     context_items: List[Dict]) -> str:\n",
    "    # context items will be from pages and chunks, the actual specific data topic\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    # Here is a prompt engineering, were doing a technique: step by step for LLM processing it's answer\n",
    "    # What helps LLM's too is by giving them examples of the way they should answer to certain questions\n",
    "    # related to certain questions, not on topic by the way being asked, format of data can have the algorithm\n",
    "    # find more relations on the underlying data\n",
    "    base_prompt = \"\"\"In this text, you will act as supportive medical assistant.\n",
    "Give yourself room to think.\n",
    "Explain each topic with facts and also suggestions based on the users needs.\n",
    "Keep your answers thorough but practical.\n",
    "You as the assistant will recieve context items for retrieving information.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1. \n",
    "Query: How does genetics effect cancer?\n",
    "Answer: Some people inherit gene mutations (changes) from their parents. These inherited mutations are though rare but can occur in BRCA1 and BRCA2 gene mutations, which are related to break or ovarian cancer.\n",
    "\\nExample 2.\n",
    "Query: Can you get heart disease if you had a stroke?\n",
    "Answer: Yes, you can get heart disease. Chances get higher if you are viewing increase in blood pressure, diabites, cholesterol or smoking. Strokes however don't directly cause heart disease but does lead to it if no treatment is done.\n",
    "\\nExample 3.\n",
    "Query: What type of treatment should you do if you get breast cancer?\n",
    "Answer: That may vary depending on the size of the cancer? If you got a response from your doctor I can help suggest you what considerations you can do. One of the main treatments you can do is getting a surgey, chemotherapy or hormonal therapy. If you give me more details on your doctors response, I can help narrow what treatment can work best according to your record and body. \n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: Please extract the context items that helped you answer the user's question\n",
    "<extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # let's format our whole prompt to be placed with context and query\n",
    "    # Augmentation step\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # We need to format it for conversation style\n",
    "    # We beed to change dialogue template since model isn't great at answering our prompt because we are saying it\n",
    "    # in a different template from what it currently has, we need to change that\n",
    "\n",
    "    # Create prompt template for conversation for instruction tuned model (the content now contains our base_prompt\n",
    "    # to give to LLM model)\n",
    "    # this will have a list that is avalaible on LLM on the model card\n",
    "    # Optimized step for LLM model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": base_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply the chat template with the tokenizer\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                           tokenize=False, # We'll do this with our tokenizer below and not here\n",
    "                                           add_generation_prompt=True)\n",
    "    \n",
    "    # Let's join them as dot points, Based on these context .... Generate thing\n",
    "    return prompt # the prompt is just the context\n",
    "\n",
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = rag_pipeline(query,\n",
    "                               embedding_model=embedding_model,\n",
    "                               embeddings=embeddings,\n",
    "                               device='cuda',\n",
    "                               chunk_min_token=chunk_min_token,\n",
    "                               )\n",
    "\n",
    "# Create a list of context items\n",
    "# each item in pages_and_chunks\n",
    "context_items = [chunk_token_dict[i] for i in indices]\n",
    "# This the augmentation it should want to view being related towards queries question, The LLM shoudl use this\n",
    "# as a resources to further answer the queries question with more depth on specific material, without general steps\n",
    "# to do something but rather direct ones\n",
    "\n",
    "# Format our prompt\n",
    "prompt = prompt_formatter(query=query, context_items=context_items)\n",
    "\n",
    "# return prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're slowly now augmenting our prompt\n",
    "\n",
    "We started basic than gave instructions, examples, room to think, step by step, we formatted the base prompt with the context and the query\n",
    "\n",
    "Now we have optimized it (used a new way of prompt which we need to another template on the conversation for chat template (the template we have is the base prompt formatted)) for the instruction tuned model, being gemma 2b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our prompt write is just a bunch of context information but not the question and explanation to it, this part the model will do based on reading the context and using processing in the LLM to explain to the user on the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [item[\"sentence_chunk\"] for item in context_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's pass this formatted prompt the LLM to summarize and find the KEY sources and text related to the queries question with important ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's relevant docs explaining temperature and sampling:\n",
    "* https://huyenchip.com/2024/01/16/sampling.html\n",
    "* https://www.promptingguide.ai/introduction/settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does the long-term impact of COVID-19 on the cardiovascular system differ from that of other viral infections\n",
      "RAG answer:\n",
      "<bos>The context does not provide any information about the long-term impact of COVID-19 on the cardiovascular system, so I cannot extract the requested information from the context.<eos>\n",
      "CPU times: total: 2.08 s\n",
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We don't have chat formatting here yet and neither the query\n",
    "# Tokenize the prompt \n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "# this will also make the token_id for generating tokens for the outputs\n",
    "# temperature increase, increases the weights of the next possible tokens, making more randomness but creative outputs\n",
    "# less is less weight but reduces randomness and chooses a more direct output for query\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             temperature=0.7, # from 0 to 1 and the lower the value, the more deterministic the text,\n",
    "                             # the higher the value, the more creative the models interpertation and ouput\n",
    "                             # This temperature is experminetal you should try and manage what can best work for your\n",
    "                             # need and goal\n",
    "                             do_sample=True, # wheter or not to use sampling. choose a different token to output next\n",
    "                             # it takes a first sequence of tokens to generate the next.\n",
    "                             # if it's set to false, it will likely pick the next token if true it will use another form that picking the next token\n",
    "                             max_new_tokens=256)\n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(token_ids=outputs[0])\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# We dont't want the prompt to be returned which is the material on context items that LLM is getting, rather replace\n",
    "# it with nothing, we just want to see the generated text\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we said earlier, that the lower the parameters a model has especially for an LLM the less concise the response will be, in this case our model can't even respond to the text given because if it can't find anything related from the pipeline facts towrads the query than it won't intervene with a response on it's own that's not high, this is where lower parameters under 7B lack as that sense of retrieve connections for what it knows in combination to what it's given in the prompt. This won't be a problem when we use the other models like gpt-3.5 or 4 or even llama.\n",
    "\n",
    "This doesn't mean gemma is bad it's just that you need to remind the reality of it's size and limitation, as of 2024, gemma is one of the best low param models you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's functionize our LLM text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages_and_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:86\u001b[0m\n",
      "File \u001b[1;32m<timed exec>:65\u001b[0m, in \u001b[0;36mrag_llm_gen\u001b[1;34m(query, embedding)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pages_and_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Function the prompt format, with the RAG augmentation and the augmentation on LLM to text Generating\n",
    "\n",
    "def rag_llm_gen(query: str,\n",
    "                embedding):\n",
    "    \"\"\"\n",
    "    We'll convert the base prompt, augment, tune the model prompt instructions, change the LLM to chat template,\n",
    "    outputs the RAG augementation, RAGs augmentation is passed to the query which is contained in the base prompt,\n",
    "    prompt augmentation is passed to tokenizer to be token_id afterwards it placed for the LLM to generate with out other\n",
    "    tuned params such as temperature and do sample with a max token count of 256 so increase creativity but not fully on output,\n",
    "    than this output generated by model is reconverted to text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def prompt_formatter(query: str,\n",
    "                         context_items: List[Dict]) -> str:\n",
    "\n",
    "        context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "    \n",
    "        base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    Make sure your answers are as explanatory as possible.\n",
    "    Use the following examples as reference for the ideal answer style.\n",
    "    \\nExample 1:\n",
    "    Query: What are the fat-soluble vitamins?\n",
    "    Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "    \\nExample 2:\n",
    "    Query: What are the causes of type 2 diabetes?\n",
    "    Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "    \\nExample 3:\n",
    "    Query: What is the importance of hydration for physical performance?\n",
    "    Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "    \\nNow use the following context items to answer the user query:\n",
    "    {context}\n",
    "    \\nRelevant passages: <extract relevant passages from the context here>\n",
    "    User query: {query}\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "        # Augmentation step\n",
    "        base_prompt = base_prompt.format(context=context, query=query)\n",
    "    \n",
    "        # Optimized step for LLM model\n",
    "        dialogue_template = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": base_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Apply the chat template with the tokenizer\n",
    "        prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                               tokenize=False, # We'll do this with our tokenizer below and not here\n",
    "                                               add_generation_prompt=True)\n",
    "        \n",
    "        # Let's join them as dot points, Based on these context .... Generate thing\n",
    "        return prompt # the prompt is just the context\n",
    "        \n",
    "    # Get relevant resources\n",
    "    scores, indices = rag_pipeline(query,\n",
    "                                   embedding_model=embedding_model,\n",
    "                                   embeddings=embeddings,\n",
    "                                   device='cuda',\n",
    "                                   chunk_min_token=chunk_min_token,\n",
    "                                   )\n",
    "    \n",
    "    # Create a list of context items\n",
    "    # each item in pages_and_chunks\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "    \n",
    "    # Format our prompt\n",
    "    prompt = prompt_formatter(query=query, context_items=context_items)\n",
    "\n",
    "    # Tokenize the prompt \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=0.7,\n",
    "                                 do_sample=True, \n",
    "                                 max_new_tokens=256)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(token_ids=outputs[0])\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")\n",
    "\n",
    "query = random.choice(query_list)\n",
    "rag_llm_gen(query, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another function\n",
    "\n",
    "Check hugging faces generation docs to find good sources, on tips and tricks to modify the generation of llm such as the type of temperatire, do_sample or guidance scale etc\n",
    "https://huggingface.co/docs/transformers/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are set on ask_gems params so we modify more the generation part on the prompt placed to the llm to have the context items, instructions or none of these, all of this can be modified simply in `ask_gemrag` params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemrag(query: str,\n",
    "        temperature: float=0.7,\n",
    "        max_new_tokens: int=256,\n",
    "        format_answer_text=True,\n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates and answer to the query based on the relevant\n",
    "    resources.\n",
    "    \"\"\"\n",
    "\n",
    "    # RETRIEVAL\n",
    "    # Get relevant resources\n",
    "    scores, indices = rag_pipeline(query,\n",
    "                                embedding_model=embedding_model,\n",
    "                                embeddings=embeddings,\n",
    "                                device='cuda',\n",
    "                                chunk_min_token=chunk_min_token,\n",
    "                                )\n",
    "\n",
    "    # Create a list of context items\n",
    "    context_items = [chunk_token_dict[i] for i in indices]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu()\n",
    "\n",
    "    # AUGMENTATION\n",
    "    # Create the prompt and format it with context items\n",
    "    # our baseline is already in the prompt_formatter but we could place that base line prompt into a txt file so\n",
    "    # we can change several lines to experiment what output completely can the model generate\n",
    "    # this will have conversation dialogue already appended to it\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "\n",
    "    # GENERATION\n",
    "    # Tokenize the prompt\n",
    "    # Our LLM is on GPU so we want the tokenize prompt to go into the LLM being in GPU\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Decode the tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "    # This outputs the prompt but let's format it so it has the instructiosn and context more layed out for LLM\n",
    "    # To understand and generate an output with instructions given in prompt generation\n",
    "\n",
    "    # Format the answer\n",
    "    if format_answer_text:\n",
    "        # Replace prompt and special tokens\n",
    "        output_text = output_text.replace(prompt, '').replace(\"<bos>\", '').replace(\"<eos>\", '')\n",
    "\n",
    "    # Only return the answer without context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "\n",
    "    # Return the answer as the text. if not set to true than it will place the output text and context items\n",
    "    # to come with that\n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do specific genetic mutations contribute to the aggressiveness of certain types of cancer?\n",
      "CPU times: total: 2.39 s\n",
      "Wall time: 2.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The context does not provide any information about specific genetic mutations that contribute to the aggressiveness of certain types of cancer, so I cannot extract the requested information from the context.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "ask_gemrag(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* RAG = powerful technique for tenerating text based on reference documents\n",
    "* Hardware user = user GPU to accelerate embedding creation and LLM generation\n",
    "    * Keep in mind the limitations on the local hardware.\n",
    "* Many open source embedding models and LLMs starting to become available, keep experimenting to find which is best  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it's dog recognizing algorithm or image what it sees to ask in conversation what people need to communicate wiht the web and connect to node s on different business to implement there structure to order things faster in more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could rerank our model responses, try different prompts, evaluate more answers, other frameworks, paramater tuning, trimming some more data that need in hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed generation, better user experience. Stream text outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our gemma may be taking to many inputs in prompt from overedistorting what we're asking it compared the examples, the model is confusing itself to answer the questions based on the example rather than the retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 03:56:15 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti   WDDM  |   00000000:17:00.0  On |                  N/A |\n",
      "| 29%   37C    P2             91W /  250W |   10951MiB /  11264MiB |     38%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2188    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      2748    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A      5052    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      7532      C   C:\\Program Files\\Python38\\python.exe        N/A      |\n",
      "|    0   N/A  N/A     10888    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11004    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     11088    C+G   ...US\\ArmouryDevice\\asus_framework.exe      N/A      |\n",
      "|    0   N/A  N/A     11880    C+G   ...ecurityApp\\MicrosoftSecurityApp.exe      N/A      |\n",
      "|    0   N/A  N/A     14584    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17436    C+G   ...on\\123.0.2420.97\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     18556    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A     19660    C+G   ...8.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     21008    C+G   ...8.0_x64__t4vj0pshhgkwm\\Telegram.exe      N/A      |\n",
      "|    0   N/A  N/A     21112    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22336    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     29912    C+G   ...les\\Elgato\\CameraHub\\Camera Hub.exe      N/A      |\n",
      "|    0   N/A  N/A     29996    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     30432    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
